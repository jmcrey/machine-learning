{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A3Q4odUxG6yl"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def load_file(path: str):\n",
    "    \"\"\" Loads the file into memory \"\"\"\n",
    "    with open(path, 'r', encoding='utf8') as fo:\n",
    "        content = fo.read()\n",
    "    return content\n",
    "\n",
    "def clean_data(content: str):\n",
    "    \"\"\" Cleans the data for tokenization \"\"\"\n",
    "    # Lowercase text and remove leading and ending newlines\n",
    "    text = content.lower().strip('\\n')\n",
    "    # Fix any encoding issues\n",
    "    text = normalize('NFD', text).encode('utf8')\n",
    "    text = text.decode('utf8')\n",
    "    # Match by paragraphs (at least two new lines)\n",
    "    par_match = re.compile(r'\\n{2,}')\n",
    "    lines = par_match.split(text)\n",
    "    cleaned = list()\n",
    "    for line in lines:\n",
    "        # Remove new line symbols\n",
    "        line = line.replace('\\n', ' ')\n",
    "        # Remove special characters and numbers\n",
    "        line = re.sub(\"[^a-z\\s\\']+\", \" \", line).replace(\"'\", \"\")\n",
    "        # Remove line with < 15 words\n",
    "        line = line.split()\n",
    "        if len(line) > 15:\n",
    "          # Remove whitespace\n",
    "          line = ' '.join(line)\n",
    "          cleaned.append(line)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjxnAM4cMBd3",
    "outputId": "4dd27535-8273-48c4-b885-ad6a58259c3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at http www gutenberg org license if you are not located in the united states you ll have to check the laws of the country where you are located before using this ebook',\n",
       "        'part i book i the history of a family chapter i fyodor pavlovitch karamazov chapter ii he gets rid of his eldest son chapter iii the second marriage and the second family chapter iv the third son alyosha chapter v elders book ii an unfortunate gathering chapter i they arrive at the monastery chapter ii the old buffoon chapter iii peasant women who have faith chapter iv a lady of little faith chapter v so be it so be it chapter vi why is such a man alive chapter vii a young man bent on a career chapter viii the scandalous scene book iii the sensualists chapter i in the servants quarters chapter ii lizaveta chapter iii the confession of a passionate heart in verse chapter iv the confession of a passionate heart in anecdote chapter v the confession of a passionate heart heels up chapter vi smerdyakov chapter vii the controversy chapter viii over the brandy chapter ix the sensualists chapter x both together chapter xi another reputation ruined part ii book iv lacerations chapter i father ferapont chapter ii at his father s chapter iii a meeting with the schoolboys chapter iv at the hohlakovs chapter v a laceration in the drawing room chapter vi a laceration in the cottage chapter vii and in the open air book v pro and contra chapter i the engagement chapter ii smerdyakov with a guitar chapter iii the brothers make friends chapter iv rebellion chapter v the grand inquisitor chapter vi for awhile a very obscure one chapter vii it s always worth while speaking to a clever man book vi the russian monk chapter i father zossima and his visitors chapter ii the duel chapter iii conversations and exhortations of father zossima part iii book vii alyosha chapter i the breath of corruption chapter ii a critical moment chapter iii an onion chapter iv cana of galilee book viii mitya chapter i kuzma samsonov chapter ii lyagavy chapter iii gold mines chapter iv in the dark chapter v a sudden resolution chapter vi i am coming too chapter vii the first and rightful lover chapter viii delirium book ix the preliminary investigation chapter i the beginning of perhotin s official career chapter ii the alarm chapter iii the sufferings of a soul the first ordeal chapter iv the second ordeal chapter v the third ordeal chapter vi the prosecutor catches mitya chapter vii mitya s great secret received with hisses chapter viii the evidence of the witnesses the babe chapter ix they carry mitya away part iv book x the boys chapter i kolya krassotkin chapter ii children chapter iii the schoolboy chapter iv the lost dog chapter v by ilusha s bedside chapter vi precocity chapter vii ilusha book xi ivan chapter i at grushenka s chapter ii the injured foot chapter iii a little demon chapter iv a hymn and a secret chapter v not you not you chapter vi the first interview with smerdyakov chapter vii the second visit to smerdyakov chapter viii the third and last interview with smerdyakov chapter ix the devil ivan s nightmare chapter x it was he who said that book xii a judicial error chapter i the fatal day chapter ii dangerous witnesses chapter iii the medical experts and a pound of nuts chapter iv fortune smiles on mitya chapter v a sudden catastrophe chapter vi the prosecutor s speech sketches of character chapter vii an historical survey chapter viii a treatise on smerdyakov chapter ix the galloping troika the end of the prosecutor s speech chapter x the speech for the defense an argument that cuts both ways chapter xi there was no money there was no robbery chapter xii and there was no murder either chapter xiii a corrupter of thought chapter xiv the peasants stand firm epilogue chapter i plans for mitya s escape chapter ii for a moment the lie becomes truth chapter iii ilusha s funeral the speech at the stone footnotes',\n",
       "        'alexey fyodorovitch karamazov was the third son of fyodor pavlovitch karamazov a land owner well known in our district in his own day and still remembered among us owing to his gloomy and tragic death which happened thirteen years ago and which i shall describe in its proper place for the present i will only say that this landowner for so we used to call him although he hardly spent a day of his life on his own estate was a strange type yet one pretty frequently to be met with a type abject and vicious and at the same time senseless but he was one of those senseless persons who are very well capable of looking after their worldly affairs and apparently after nothing else fyodor pavlovitch for instance began with next to nothing his estate was of the smallest he ran to dine at other men s tables and fastened on them as a toady yet at his death it appeared that he had a hundred thousand roubles in hard cash at the same time he was all his life one of the most senseless fantastical fellows in the whole district i repeat it was not stupidity the majority of these fantastical fellows are shrewd and intelligent enough but just senselessness and a peculiar national form of it',\n",
       "        'he was married twice and had three sons the eldest dmitri by his first wife and two ivan and alexey by his second fyodor pavlovitch s first wife adelai da ivanovna belonged to a fairly rich and distinguished noble family also landowners in our district the miu sovs how it came to pass that an heiress who was also a beauty and moreover one of those vigorous intelligent girls so common in this generation but sometimes also to be found in the last could have married such a worthless puny weakling as we all called him i won t attempt to explain i knew a young lady of the last romantic generation who after some years of an enigmatic passion for a gentleman whom she might quite easily have married at any moment invented insuperable obstacles to their union and ended by throwing herself one stormy night into a rather deep and rapid river from a high bank almost a precipice and so perished entirely to satisfy her own caprice and to be like shakespeare s ophelia indeed if this precipice a chosen and favorite spot of hers had been less picturesque if there had been a prosaic flat bank in its place most likely the suicide would never have taken place this is a fact and probably there have been not a few similar instances in the last two or three generations adelai da ivanovna miu sov s action was similarly no doubt an echo of other people s ideas and was due to the irritation caused by lack of mental freedom she wanted perhaps to show her feminine independence to override class distinctions and the despotism of her family and a pliable imagination persuaded her we must suppose for a brief moment that fyodor pavlovitch in spite of his parasitic position was one of the bold and ironical spirits of that progressive epoch though he was in fact an ill natured buffoon and nothing more what gave the marriage piquancy was that it was preceded by an elopement and this greatly captivated adelai da ivanovna s fancy fyodor pavlovitch s position at the time made him specially eager for any such enterprise for he was passionately anxious to make a career in one way or another to attach himself to a good family and obtain a dowry was an alluring prospect as for mutual love it did not exist apparently either in the bride or in him in spite of adelai da ivanovna s beauty this was perhaps a unique case of the kind in the life of fyodor pavlovitch who was always of a voluptuous temper and ready to run after any petticoat on the slightest encouragement she seems to have been the only woman who made no particular appeal to his senses',\n",
       "        'immediately after the elopement adelai da ivanovna discerned in a flash that she had no feeling for her husband but contempt the marriage accordingly showed itself in its true colors with extraordinary rapidity although the family accepted the event pretty quickly and apportioned the runaway bride her dowry the husband and wife began to lead a most disorderly life and there were everlasting scenes between them it was said that the young wife showed incomparably more generosity and dignity than fyodor pavlovitch who as is now known got hold of all her money up to twenty five thousand roubles as soon as she received it so that those thousands were lost to her for ever the little village and the rather fine town house which formed part of her dowry he did his utmost for a long time to transfer to his name by means of some deed of conveyance he would probably have succeeded merely from her moral fatigue and desire to get rid of him and from the contempt and loathing he aroused by his persistent and shameless importunity but fortunately adelai da ivanovna s family intervened and circumvented his greediness it is known for a fact that frequent fights took place between the husband and wife but rumor had it that fyodor pavlovitch did not beat his wife but was beaten by her for she was a hot tempered bold dark browed impatient woman possessed of remarkable physical strength finally she left the house and ran away from fyodor pavlovitch with a destitute divinity student leaving mitya a child of three years old in her husband s hands immediately fyodor pavlovitch introduced a regular harem into the house and abandoned himself to orgies of drunkenness in the intervals he used to drive all over the province complaining tearfully to each and all of adelai da ivanovna s having left him going into details too disgraceful for a husband to mention in regard to his own married life what seemed to gratify him and flatter his self love most was to play the ridiculous part of the injured husband and to parade his woes with embellishments',\n",
       "        'one would think that you d got a promotion fyodor pavlovitch you seem so pleased in spite of your sorrow scoffers said to him many even added that he was glad of a new comic part in which to play the buffoon and that it was simply to make it funnier that he pretended to be unaware of his ludicrous position but who knows it may have been simplicity at last he succeeded in getting on the track of his runaway wife the poor woman turned out to be in petersburg where she had gone with her divinity student and where she had thrown herself into a life of complete emancipation fyodor pavlovitch at once began bustling about making preparations to go to petersburg with what object he could not himself have said he would perhaps have really gone but having determined to do so he felt at once entitled to fortify himself for the journey by another bout of reckless drinking and just at that time his wife s family received the news of her death in petersburg she had died quite suddenly in a garret according to one story of typhus or as another version had it of starvation fyodor pavlovitch was drunk when he heard of his wife s death and the story is that he ran out into the street and began shouting with joy raising his hands to heaven lord now lettest thou thy servant depart in peace but others say he wept without restraint like a little child so much so that people were sorry for him in spite of the repulsion he inspired it is quite possible that both versions were true that he rejoiced at his release and at the same time wept for her who released him as a general rule people even the wicked are much more nai ve and simple hearted than we suppose and we ourselves are too',\n",
       "        'you can easily imagine what a father such a man could be and how he would bring up his children his behavior as a father was exactly what might be expected he completely abandoned the child of his marriage with adelai da ivanovna not from malice nor because of his matrimonial grievances but simply because he forgot him while he was wearying every one with his tears and complaints and turning his house into a sink of debauchery a faithful servant of the family grigory took the three year old mitya into his care if he hadn t looked after him there would have been no one even to change the baby s little shirt',\n",
       "        'it happened moreover that the child s relations on his mother s side forgot him too at first his grandfather was no longer living his widow mitya s grandmother had moved to moscow and was seriously ill while his daughters were married so that mitya remained for almost a whole year in old grigory s charge and lived with him in the servant s cottage but if his father had remembered him he could not indeed have been altogether unaware of his existence he would have sent him back to the cottage as the child would only have been in the way of his debaucheries but a cousin of mitya s mother pyotr alexandrovitch miu sov happened to return from paris he lived for many years afterwards abroad but was at that time quite a young man and distinguished among the miu sovs as a man of enlightened ideas and of european culture who had been in the capitals and abroad towards the end of his life he became a liberal of the type common in the forties and fifties in the course of his career he had come into contact with many of the most liberal men of his epoch both in russia and abroad he had known proudhon and bakunin personally and in his declining years was very fond of describing the three days of the paris revolution of february hinting that he himself had almost taken part in the fighting on the barricades this was one of the most grateful recollections of his youth he had an independent property of about a thousand souls to reckon in the old style his splendid estate lay on the outskirts of our little town and bordered on the lands of our famous monastery with which pyotr alexandrovitch began an endless lawsuit almost as soon as he came into the estate concerning the rights of fishing in the river or wood cutting in the forest i don t know exactly which he regarded it as his duty as a citizen and a man of culture to open an attack upon the clericals hearing all about adelai da ivanovna whom he of course remembered and in whom he had at one time been interested and learning of the existence of mitya he intervened in spite of all his youthful indignation and contempt for fyodor pavlovitch he made the latter s acquaintance for the first time and told him directly that he wished to undertake the child s education he used long afterwards to tell as a characteristic touch that when he began to speak of mitya fyodor pavlovitch looked for some time as though he did not understand what child he was talking about and even as though he was surprised to hear that he had a little son in the house the story may have been exaggerated yet it must have been something like the truth',\n",
       "        'fyodor pavlovitch was all his life fond of acting of suddenly playing an unexpected part sometimes without any motive for doing so and even to his own direct disadvantage as for instance in the present case this habit however is characteristic of a very great number of people some of them very clever ones not like fyodor pavlovitch pyotr alexandrovitch carried the business through vigorously and was appointed with fyodor pavlovitch joint guardian of the child who had a small property a house and land left him by his mother mitya did in fact pass into this cousin s keeping but as the latter had no family of his own and after securing the revenues of his estates was in haste to return at once to paris he left the boy in charge of one of his cousins a lady living in moscow it came to pass that settling permanently in paris he too forgot the child especially when the revolution of february broke out making an impression on his mind that he remembered all the rest of his life the moscow lady died and mitya passed into the care of one of her married daughters i believe he changed his home a fourth time later on i won t enlarge upon that now as i shall have much to tell later of fyodor pavlovitch s firstborn and must confine myself now to the most essential facts about him without which i could not begin my story',\n",
       "        'in the first place this mitya or rather dmitri fyodorovitch was the only one of fyodor pavlovitch s three sons who grew up in the belief that he had property and that he would be independent on coming of age he spent an irregular boyhood and youth he did not finish his studies at the gymnasium he got into a military school then went to the caucasus was promoted fought a duel and was degraded to the ranks earned promotion again led a wild life and spent a good deal of money he did not begin to receive any income from fyodor pavlovitch until he came of age and until then got into debt he saw and knew his father fyodor pavlovitch for the first time on coming of age when he visited our neighborhood on purpose to settle with him about his property he seems not to have liked his father he did not stay long with him and made haste to get away having only succeeded in obtaining a sum of money and entering into an agreement for future payments from the estate of the revenues and value of which he was unable a fact worthy of note upon this occasion to get a statement from his father fyodor pavlovitch remarked for the first time then this too should be noted that mitya had a vague and exaggerated idea of his property fyodor pavlovitch was very well satisfied with this as it fell in with his own designs he gathered only that the young man was frivolous unruly of violent passions impatient and dissipated and that if he could only obtain ready money he would be satisfied although only of course for a short time so fyodor pavlovitch began to take advantage of this fact sending him from time to time small doles installments in the end when four years later mitya losing patience came a second time to our little town to settle up once for all with his father it turned out to his amazement that he had nothing that it was difficult to get an account even that he had received the whole value of his property in sums of money from fyodor pavlovitch and was perhaps even in debt to him that by various agreements into which he had of his own desire entered at various previous dates he had no right to expect anything more and so on and so on the young man was overwhelmed suspected deceit and cheating and was almost beside himself and indeed this circumstance led to the catastrophe the account of which forms the subject of my first introductory story or rather the external side of it but before i pass to that story i must say a little of fyodor pavlovitch s other two sons and of their origin'],\n",
       "       dtype='<U9487'),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fyodor = load_file('q1/28054-0.txt')\n",
    "f_data = clean_data(fyodor)\n",
    "f_labels = np.zeros(f_data.shape[0])\n",
    "f_data[0:10], f_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWYyykKGjNQi",
    "outputId": "b761e5e2-f3d7-4125-891f-f9ffdec25152"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg net',\n",
       "        'i a scandal in bohemia ii the red headed league iii a case of identity iv the boscombe valley mystery v the five orange pips vi the man with the twisted lip vii the adventure of the blue carbuncle viii the adventure of the speckled band ix the adventure of the engineers thumb x the adventure of the noble bachelor xi the adventure of the beryl coronet xii the adventure of the copper beeches',\n",
       "        'to sherlock holmes she is always the woman i have seldom heard him mention her under any other name in his eyes she eclipses and predominates the whole of her sex it was not that he felt any emotion akin to love for irene adler all emotions and that one particularly were abhorrent to his cold precise but admirably balanced mind he was i take it the most perfect reasoning and observing machine that the world has seen but as a lover he would have placed himself in a false position he never spoke of the softer passions save with a gibe and a sneer they were admirable things for the observer excellent for drawing the veil from mens motives and actions but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results grit in a sensitive instrument or a crack in one of his own high power lenses would not be more disturbing than a strong emotion in a nature such as his and yet there was but one woman to him and that woman was the late irene adler of dubious and questionable memory',\n",
       "        'i had seen little of holmes lately my marriage had drifted us away from each other my own complete happiness and the home centred interests which rise up around the man who first finds himself master of his own establishment were sufficient to absorb all my attention while holmes who loathed every form of society with his whole bohemian soul remained in our lodgings in baker street buried among his old books and alternating from week to week between cocaine and ambition the drowsiness of the drug and the fierce energy of his own keen nature he was still as ever deeply attracted by the study of crime and occupied his immense faculties and extraordinary powers of observation in following out those clues and clearing up those mysteries which had been abandoned as hopeless by the official police from time to time i heard some vague account of his doings of his summons to odessa in the case of the trepoff murder of his clearing up of the singular tragedy of the atkinson brothers at trincomalee and finally of the mission which he had accomplished so delicately and successfully for the reigning family of holland beyond these signs of his activity however which i merely shared with all the readers of the daily press i knew little of my former friend and companion',\n",
       "        'one night it was on the twentieth of march i was returning from a journey to a patient for i had now returned to civil practice when my way led me through baker street as i passed the well remembered door which must always be associated in my mind with my wooing and with the dark incidents of the study in scarlet i was seized with a keen desire to see holmes again and to know how he was employing his extraordinary powers his rooms were brilliantly lit and even as i looked up i saw his tall spare figure pass twice in a dark silhouette against the blind he was pacing the room swiftly eagerly with his head sunk upon his chest and his hands clasped behind him to me who knew his every mood and habit his attitude and manner told their own story he was at work again he had risen out of his drug created dreams and was hot upon the scent of some new problem i rang the bell and was shown up to the chamber which had formerly been in part my own',\n",
       "        'his manner was not effusive it seldom was but he was glad i think to see me with hardly a word spoken but with a kindly eye he waved me to an armchair threw across his case of cigars and indicated a spirit case and a gasogene in the corner then he stood before the fire and looked me over in his singular introspective fashion',\n",
       "        'wedlock suits you he remarked i think watson that you have put on seven and a half pounds since i saw you',\n",
       "        'indeed i should have thought a little more just a trifle more i fancy watson and in practice again i observe you did not tell me that you intended to go into harness',\n",
       "        'i see it i deduce it how do i know that you have been getting yourself very wet lately and that you have a most clumsy and careless servant girl',\n",
       "        'my dear holmes said i this is too much you would certainly have been burned had you lived a few centuries ago it is true that i had a country walk on thursday and came home in a dreadful mess but as i have changed my clothes i cant imagine how you deduce it as to mary jane she is incorrigible and my wife has given her notice but there again i fail to see how you work it out'],\n",
       "       dtype='<U2489'),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doyle = load_file('q1/pg1661.txt')\n",
    "d_data = clean_data(doyle)\n",
    "d_labels = np.ones(d_data.shape[0])\n",
    "d_data[0:10], d_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nrn59VrnjrW3",
    "outputId": "8e6fd3f3-a805-4347-d939-cbcd1d760dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg org',\n",
       "        'note the accompanying html file has active links to all the volumes and chapters in this set',\n",
       "        'sir walter elliot of kellynch hall in somersetshire was a man who for his own amusement never took up any book but the baronetage there he found occupation for an idle hour and consolation in a distressed one there his faculties were roused into admiration and respect by contemplating the limited remnant of the earliest patents there any unwelcome sensations arising from domestic affairs changed naturally into pity and contempt as he turned over the almost endless creations of the last century and there if every other leaf were powerless he could read his own history with an interest which never failed this was the page at which the favourite volume always opened',\n",
       "        'walter elliot born march married july elizabeth daughter of james stevenson esq of south park in the county of gloucester by which lady who died he has issue elizabeth born june anne born august a still born son november mary born november',\n",
       "        'precisely such had the paragraph originally stood from the printers hands but sir walter had improved it by adding for the information of himself and his family these words after the date of marys birth married december charles son and heir of charles musgrove esq of uppercross in the county of somerset and by inserting most accurately the day of the month on which he had lost his wife',\n",
       "        'then followed the history and rise of the ancient and respectable family in the usual terms how it had been first settled in cheshire how mentioned in dugdale serving the office of high sheriff representing a borough in three successive parliaments exertions of loyalty and dignity of baronet in the first year of charles ii with all the marys and elizabeths they had married forming altogether two handsome duodecimo pages and concluding with the arms and motto principal seat kellynch hall in the county of somerset and sir walters handwriting again in this finale',\n",
       "        'vanity was the beginning and the end of sir walter elliots character vanity of person and of situation he had been remarkably handsome in his youth and at fifty four was still a very fine man few women could think more of their personal appearance than he did nor could the valet of any new made lord be more delighted with the place he held in society he considered the blessing of beauty as inferior only to the blessing of a baronetcy and the sir walter elliot who united these gifts was the constant object of his warmest respect and devotion',\n",
       "        'his good looks and his rank had one fair claim on his attachment since to them he must have owed a wife of very superior character to any thing deserved by his own lady elliot had been an excellent woman sensible and amiable whose judgement and conduct if they might be pardoned the youthful infatuation which made her lady elliot had never required indulgence afterwards she had humoured or softened or concealed his failings and promoted his real respectability for seventeen years and though not the very happiest being in the world herself had found enough in her duties her friends and her children to attach her to life and make it no matter of indifference to her when she was called on to quit them three girls the two eldest sixteen and fourteen was an awful legacy for a mother to bequeath an awful charge rather to confide to the authority and guidance of a conceited silly father she had however one very intimate friend a sensible deserving woman who had been brought by strong attachment to herself to settle close by her in the village of kellynch and on her kindness and advice lady elliot mainly relied for the best help and maintenance of the good principles and instruction which she had been anxiously giving her daughters',\n",
       "        'this friend and sir walter did not marry whatever might have been anticipated on that head by their acquaintance thirteen years had passed away since lady elliots death and they were still near neighbours and intimate friends and one remained a widower the other a widow',\n",
       "        'that lady russell of steady age and character and extremely well provided for should have no thought of a second marriage needs no apology to the public which is rather apt to be unreasonably discontented when a woman does marry again than when she does not but sir walters continuing in singleness requires explanation be it known then that sir walter like a good father having met with one or two private disappointments in very unreasonable applications prided himself on remaining single for his dear daughters sake for one daughter his eldest he would really have given up any thing which he had not been very much tempted to do elizabeth had succeeded at sixteen to all that was possible of her mothers rights and consequence and being very handsome and very like himself her influence had always been great and they had gone on together most happily his two other children were of very inferior value mary had acquired a little artificial importance by becoming mrs charles musgrove but anne with an elegance of mind and sweetness of character which must have placed her high with any people of real understanding was nobody with either father or sister her word had no weight her convenience was always to give way she was only anne',\n",
       "        'to lady russell indeed she was a most dear and highly valued god daughter favourite and friend lady russell loved them all but it was only in anne that she could fancy the mother to revive again',\n",
       "        'a few years before anne elliot had been a very pretty girl but her bloom had vanished early and as even in its height her father had found little to admire in her so totally different were her delicate features and mild dark eyes from his own there could be nothing in them now that she was faded and thin to excite his esteem he had never indulged much hope he had now none of ever reading her name in any other page of his favourite work all equality of alliance must rest with elizabeth for mary had merely connected herself with an old country family of respectability and large fortune and had therefore given all the honour and received none elizabeth would one day or other marry suitably',\n",
       "        'it sometimes happens that a woman is handsomer at twenty nine than she was ten years before and generally speaking if there has been neither ill health nor anxiety it is a time of life at which scarcely any charm is lost it was so with elizabeth still the same handsome miss elliot that she had begun to be thirteen years ago and sir walter might be excused therefore in forgetting her age or at least be deemed only half a fool for thinking himself and elizabeth as blooming as ever amidst the wreck of the good looks of everybody else for he could plainly see how old all the rest of his family and acquaintance were growing anne haggard mary coarse every face in the neighbourhood worsting and the rapid increase of the crows foot about lady russells temples had long been a distress to him',\n",
       "        'elizabeth did not quite equal her father in personal contentment thirteen years had seen her mistress of kellynch hall presiding and directing with a self possession and decision which could never have given the idea of her being younger than she was for thirteen years had she been doing the honours and laying down the domestic law at home and leading the way to the chaise and four and walking immediately after lady russell out of all the drawing rooms and dining rooms in the country thirteen winters revolving frosts had seen her opening every ball of credit which a scanty neighbourhood afforded and thirteen springs shewn their blossoms as she travelled up to london with her father for a few weeks annual enjoyment of the great world she had the remembrance of all this she had the consciousness of being nine and twenty to give her some regrets and some apprehensions she was fully satisfied of being still quite as handsome as ever but she felt her approach to the years of danger and would have rejoiced to be certain of being properly solicited by baronet blood within the next twelvemonth or two then might she again take up the book of books with as much enjoyment as in her early youth but now she liked it not always to be presented with the date of her own birth and see no marriage follow but that of a youngest sister made the book an evil and more than once when her father had left it open on the table near her had she closed it with averted eyes and pushed it away',\n",
       "        'she had had a disappointment moreover which that book and especially the history of her own family must ever present the remembrance of the heir presumptive the very william walter elliot esq whose rights had been so generously supported by her father had disappointed her',\n",
       "        'she had while a very young girl as soon as she had known him to be in the event of her having no brother the future baronet meant to marry him and her father had always meant that she should he had not been known to them as a boy but soon after lady elliots death sir walter had sought the acquaintance and though his overtures had not been met with any warmth he had persevered in seeking it making allowance for the modest drawing back of youth and in one of their spring excursions to london when elizabeth was in her first bloom mr elliot had been forced into the introduction',\n",
       "        'he was at that time a very young man just engaged in the study of the law and elizabeth found him extremely agreeable and every plan in his favour was confirmed he was invited to kellynch hall he was talked of and expected all the rest of the year but he never came the following spring he was seen again in town found equally agreeable again encouraged invited and expected and again he did not come and the next tidings were that he was married instead of pushing his fortune in the line marked out for the heir of the house of elliot he had purchased independence by uniting himself to a rich woman of inferior birth',\n",
       "        'sir walter has resented it as the head of the house he felt that he ought to have been consulted especially after taking the young man so publicly by the hand for they must have been seen together he observed once at tattersalls and twice in the lobby of the house of commons his disapprobation was expressed but apparently very little regarded mr elliot had attempted no apology and shewn himself as unsolicitous of being longer noticed by the family as sir walter considered him unworthy of it all acquaintance between them had ceased',\n",
       "        'this very awkward history of mr elliot was still after an interval of several years felt with anger by elizabeth who had liked the man for himself and still more for being her fathers heir and whose strong family pride could see only in him a proper match for sir walter elliots eldest daughter there was not a baronet from a to z whom her feelings could have so willingly acknowledged as an equal yet so miserably had he conducted himself that though she was at this present time the summer of wearing black ribbons for his wife she could not admit him to be worth thinking of again the disgrace of his first marriage might perhaps as there was no reason to suppose it perpetuated by offspring have been got over had he not done worse but he had as by the accustomary intervention of kind friends they had been informed spoken most disrespectfully of them all most slightingly and contemptuously of the very blood he belonged to and the honours which were hereafter to be his own this could not be pardoned',\n",
       "        'such were elizabeth elliots sentiments and sensations such the cares to alloy the agitations to vary the sameness and the elegance the prosperity and the nothingness of her scene of life such the feelings to give interest to a long uneventful residence in one country circle to fill the vacancies which there were no habits of utility abroad no talents or accomplishments for home to occupy'],\n",
       "       dtype='<U14176'),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen = load_file('q1/pg31100.txt')\n",
    "a_data = clean_data(austen)\n",
    "a_labels = np.ones(a_data.shape[0]) + 1\n",
    "a_data[0:20], a_labels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8j0fIJ6-kOOu",
    "outputId": "e2e7ed3c-def9-48a2-a29d-160688a50dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at http www gutenberg org license if you are not located in the united states you ll have to check the laws of the country where you are located before using this ebook',\n",
       "        'part i book i the history of a family chapter i fyodor pavlovitch karamazov chapter ii he gets rid of his eldest son chapter iii the second marriage and the second family chapter iv the third son alyosha chapter v elders book ii an unfortunate gathering chapter i they arrive at the monastery chapter ii the old buffoon chapter iii peasant women who have faith chapter iv a lady of little faith chapter v so be it so be it chapter vi why is such a man alive chapter vii a young man bent on a career chapter viii the scandalous scene book iii the sensualists chapter i in the servants quarters chapter ii lizaveta chapter iii the confession of a passionate heart in verse chapter iv the confession of a passionate heart in anecdote chapter v the confession of a passionate heart heels up chapter vi smerdyakov chapter vii the controversy chapter viii over the brandy chapter ix the sensualists chapter x both together chapter xi another reputation ruined part ii book iv lacerations chapter i father ferapont chapter ii at his father s chapter iii a meeting with the schoolboys chapter iv at the hohlakovs chapter v a laceration in the drawing room chapter vi a laceration in the cottage chapter vii and in the open air book v pro and contra chapter i the engagement chapter ii smerdyakov with a guitar chapter iii the brothers make friends chapter iv rebellion chapter v the grand inquisitor chapter vi for awhile a very obscure one chapter vii it s always worth while speaking to a clever man book vi the russian monk chapter i father zossima and his visitors chapter ii the duel chapter iii conversations and exhortations of father zossima part iii book vii alyosha chapter i the breath of corruption chapter ii a critical moment chapter iii an onion chapter iv cana of galilee book viii mitya chapter i kuzma samsonov chapter ii lyagavy chapter iii gold mines chapter iv in the dark chapter v a sudden resolution chapter vi i am coming too chapter vii the first and rightful lover chapter viii delirium book ix the preliminary investigation chapter i the beginning of perhotin s official career chapter ii the alarm chapter iii the sufferings of a soul the first ordeal chapter iv the second ordeal chapter v the third ordeal chapter vi the prosecutor catches mitya chapter vii mitya s great secret received with hisses chapter viii the evidence of the witnesses the babe chapter ix they carry mitya away part iv book x the boys chapter i kolya krassotkin chapter ii children chapter iii the schoolboy chapter iv the lost dog chapter v by ilusha s bedside chapter vi precocity chapter vii ilusha book xi ivan chapter i at grushenka s chapter ii the injured foot chapter iii a little demon chapter iv a hymn and a secret chapter v not you not you chapter vi the first interview with smerdyakov chapter vii the second visit to smerdyakov chapter viii the third and last interview with smerdyakov chapter ix the devil ivan s nightmare chapter x it was he who said that book xii a judicial error chapter i the fatal day chapter ii dangerous witnesses chapter iii the medical experts and a pound of nuts chapter iv fortune smiles on mitya chapter v a sudden catastrophe chapter vi the prosecutor s speech sketches of character chapter vii an historical survey chapter viii a treatise on smerdyakov chapter ix the galloping troika the end of the prosecutor s speech chapter x the speech for the defense an argument that cuts both ways chapter xi there was no money there was no robbery chapter xii and there was no murder either chapter xiii a corrupter of thought chapter xiv the peasants stand firm epilogue chapter i plans for mitya s escape chapter ii for a moment the lie becomes truth chapter iii ilusha s funeral the speech at the stone footnotes',\n",
       "        'alexey fyodorovitch karamazov was the third son of fyodor pavlovitch karamazov a land owner well known in our district in his own day and still remembered among us owing to his gloomy and tragic death which happened thirteen years ago and which i shall describe in its proper place for the present i will only say that this landowner for so we used to call him although he hardly spent a day of his life on his own estate was a strange type yet one pretty frequently to be met with a type abject and vicious and at the same time senseless but he was one of those senseless persons who are very well capable of looking after their worldly affairs and apparently after nothing else fyodor pavlovitch for instance began with next to nothing his estate was of the smallest he ran to dine at other men s tables and fastened on them as a toady yet at his death it appeared that he had a hundred thousand roubles in hard cash at the same time he was all his life one of the most senseless fantastical fellows in the whole district i repeat it was not stupidity the majority of these fantastical fellows are shrewd and intelligent enough but just senselessness and a peculiar national form of it',\n",
       "        'he was married twice and had three sons the eldest dmitri by his first wife and two ivan and alexey by his second fyodor pavlovitch s first wife adelai da ivanovna belonged to a fairly rich and distinguished noble family also landowners in our district the miu sovs how it came to pass that an heiress who was also a beauty and moreover one of those vigorous intelligent girls so common in this generation but sometimes also to be found in the last could have married such a worthless puny weakling as we all called him i won t attempt to explain i knew a young lady of the last romantic generation who after some years of an enigmatic passion for a gentleman whom she might quite easily have married at any moment invented insuperable obstacles to their union and ended by throwing herself one stormy night into a rather deep and rapid river from a high bank almost a precipice and so perished entirely to satisfy her own caprice and to be like shakespeare s ophelia indeed if this precipice a chosen and favorite spot of hers had been less picturesque if there had been a prosaic flat bank in its place most likely the suicide would never have taken place this is a fact and probably there have been not a few similar instances in the last two or three generations adelai da ivanovna miu sov s action was similarly no doubt an echo of other people s ideas and was due to the irritation caused by lack of mental freedom she wanted perhaps to show her feminine independence to override class distinctions and the despotism of her family and a pliable imagination persuaded her we must suppose for a brief moment that fyodor pavlovitch in spite of his parasitic position was one of the bold and ironical spirits of that progressive epoch though he was in fact an ill natured buffoon and nothing more what gave the marriage piquancy was that it was preceded by an elopement and this greatly captivated adelai da ivanovna s fancy fyodor pavlovitch s position at the time made him specially eager for any such enterprise for he was passionately anxious to make a career in one way or another to attach himself to a good family and obtain a dowry was an alluring prospect as for mutual love it did not exist apparently either in the bride or in him in spite of adelai da ivanovna s beauty this was perhaps a unique case of the kind in the life of fyodor pavlovitch who was always of a voluptuous temper and ready to run after any petticoat on the slightest encouragement she seems to have been the only woman who made no particular appeal to his senses',\n",
       "        'immediately after the elopement adelai da ivanovna discerned in a flash that she had no feeling for her husband but contempt the marriage accordingly showed itself in its true colors with extraordinary rapidity although the family accepted the event pretty quickly and apportioned the runaway bride her dowry the husband and wife began to lead a most disorderly life and there were everlasting scenes between them it was said that the young wife showed incomparably more generosity and dignity than fyodor pavlovitch who as is now known got hold of all her money up to twenty five thousand roubles as soon as she received it so that those thousands were lost to her for ever the little village and the rather fine town house which formed part of her dowry he did his utmost for a long time to transfer to his name by means of some deed of conveyance he would probably have succeeded merely from her moral fatigue and desire to get rid of him and from the contempt and loathing he aroused by his persistent and shameless importunity but fortunately adelai da ivanovna s family intervened and circumvented his greediness it is known for a fact that frequent fights took place between the husband and wife but rumor had it that fyodor pavlovitch did not beat his wife but was beaten by her for she was a hot tempered bold dark browed impatient woman possessed of remarkable physical strength finally she left the house and ran away from fyodor pavlovitch with a destitute divinity student leaving mitya a child of three years old in her husband s hands immediately fyodor pavlovitch introduced a regular harem into the house and abandoned himself to orgies of drunkenness in the intervals he used to drive all over the province complaining tearfully to each and all of adelai da ivanovna s having left him going into details too disgraceful for a husband to mention in regard to his own married life what seemed to gratify him and flatter his self love most was to play the ridiculous part of the injured husband and to parade his woes with embellishments',\n",
       "        'one would think that you d got a promotion fyodor pavlovitch you seem so pleased in spite of your sorrow scoffers said to him many even added that he was glad of a new comic part in which to play the buffoon and that it was simply to make it funnier that he pretended to be unaware of his ludicrous position but who knows it may have been simplicity at last he succeeded in getting on the track of his runaway wife the poor woman turned out to be in petersburg where she had gone with her divinity student and where she had thrown herself into a life of complete emancipation fyodor pavlovitch at once began bustling about making preparations to go to petersburg with what object he could not himself have said he would perhaps have really gone but having determined to do so he felt at once entitled to fortify himself for the journey by another bout of reckless drinking and just at that time his wife s family received the news of her death in petersburg she had died quite suddenly in a garret according to one story of typhus or as another version had it of starvation fyodor pavlovitch was drunk when he heard of his wife s death and the story is that he ran out into the street and began shouting with joy raising his hands to heaven lord now lettest thou thy servant depart in peace but others say he wept without restraint like a little child so much so that people were sorry for him in spite of the repulsion he inspired it is quite possible that both versions were true that he rejoiced at his release and at the same time wept for her who released him as a general rule people even the wicked are much more nai ve and simple hearted than we suppose and we ourselves are too',\n",
       "        'you can easily imagine what a father such a man could be and how he would bring up his children his behavior as a father was exactly what might be expected he completely abandoned the child of his marriage with adelai da ivanovna not from malice nor because of his matrimonial grievances but simply because he forgot him while he was wearying every one with his tears and complaints and turning his house into a sink of debauchery a faithful servant of the family grigory took the three year old mitya into his care if he hadn t looked after him there would have been no one even to change the baby s little shirt',\n",
       "        'it happened moreover that the child s relations on his mother s side forgot him too at first his grandfather was no longer living his widow mitya s grandmother had moved to moscow and was seriously ill while his daughters were married so that mitya remained for almost a whole year in old grigory s charge and lived with him in the servant s cottage but if his father had remembered him he could not indeed have been altogether unaware of his existence he would have sent him back to the cottage as the child would only have been in the way of his debaucheries but a cousin of mitya s mother pyotr alexandrovitch miu sov happened to return from paris he lived for many years afterwards abroad but was at that time quite a young man and distinguished among the miu sovs as a man of enlightened ideas and of european culture who had been in the capitals and abroad towards the end of his life he became a liberal of the type common in the forties and fifties in the course of his career he had come into contact with many of the most liberal men of his epoch both in russia and abroad he had known proudhon and bakunin personally and in his declining years was very fond of describing the three days of the paris revolution of february hinting that he himself had almost taken part in the fighting on the barricades this was one of the most grateful recollections of his youth he had an independent property of about a thousand souls to reckon in the old style his splendid estate lay on the outskirts of our little town and bordered on the lands of our famous monastery with which pyotr alexandrovitch began an endless lawsuit almost as soon as he came into the estate concerning the rights of fishing in the river or wood cutting in the forest i don t know exactly which he regarded it as his duty as a citizen and a man of culture to open an attack upon the clericals hearing all about adelai da ivanovna whom he of course remembered and in whom he had at one time been interested and learning of the existence of mitya he intervened in spite of all his youthful indignation and contempt for fyodor pavlovitch he made the latter s acquaintance for the first time and told him directly that he wished to undertake the child s education he used long afterwards to tell as a characteristic touch that when he began to speak of mitya fyodor pavlovitch looked for some time as though he did not understand what child he was talking about and even as though he was surprised to hear that he had a little son in the house the story may have been exaggerated yet it must have been something like the truth',\n",
       "        'fyodor pavlovitch was all his life fond of acting of suddenly playing an unexpected part sometimes without any motive for doing so and even to his own direct disadvantage as for instance in the present case this habit however is characteristic of a very great number of people some of them very clever ones not like fyodor pavlovitch pyotr alexandrovitch carried the business through vigorously and was appointed with fyodor pavlovitch joint guardian of the child who had a small property a house and land left him by his mother mitya did in fact pass into this cousin s keeping but as the latter had no family of his own and after securing the revenues of his estates was in haste to return at once to paris he left the boy in charge of one of his cousins a lady living in moscow it came to pass that settling permanently in paris he too forgot the child especially when the revolution of february broke out making an impression on his mind that he remembered all the rest of his life the moscow lady died and mitya passed into the care of one of her married daughters i believe he changed his home a fourth time later on i won t enlarge upon that now as i shall have much to tell later of fyodor pavlovitch s firstborn and must confine myself now to the most essential facts about him without which i could not begin my story',\n",
       "        'in the first place this mitya or rather dmitri fyodorovitch was the only one of fyodor pavlovitch s three sons who grew up in the belief that he had property and that he would be independent on coming of age he spent an irregular boyhood and youth he did not finish his studies at the gymnasium he got into a military school then went to the caucasus was promoted fought a duel and was degraded to the ranks earned promotion again led a wild life and spent a good deal of money he did not begin to receive any income from fyodor pavlovitch until he came of age and until then got into debt he saw and knew his father fyodor pavlovitch for the first time on coming of age when he visited our neighborhood on purpose to settle with him about his property he seems not to have liked his father he did not stay long with him and made haste to get away having only succeeded in obtaining a sum of money and entering into an agreement for future payments from the estate of the revenues and value of which he was unable a fact worthy of note upon this occasion to get a statement from his father fyodor pavlovitch remarked for the first time then this too should be noted that mitya had a vague and exaggerated idea of his property fyodor pavlovitch was very well satisfied with this as it fell in with his own designs he gathered only that the young man was frivolous unruly of violent passions impatient and dissipated and that if he could only obtain ready money he would be satisfied although only of course for a short time so fyodor pavlovitch began to take advantage of this fact sending him from time to time small doles installments in the end when four years later mitya losing patience came a second time to our little town to settle up once for all with his father it turned out to his amazement that he had nothing that it was difficult to get an account even that he had received the whole value of his property in sums of money from fyodor pavlovitch and was perhaps even in debt to him that by various agreements into which he had of his own desire entered at various previous dates he had no right to expect anything more and so on and so on the young man was overwhelmed suspected deceit and cheating and was almost beside himself and indeed this circumstance led to the catastrophe the account of which forms the subject of my first introductory story or rather the external side of it but before i pass to that story i must say a little of fyodor pavlovitch s other two sons and of their origin'],\n",
       "       dtype='<U14176'),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.hstack((f_data, d_data))\n",
    "data = np.hstack((data, a_data))\n",
    "\n",
    "n = f_data.shape[0] + d_data.shape[0] + a_data.shape[0]\n",
    "assert data.shape[0] == n\n",
    "\n",
    "labels = np.hstack((f_labels, d_labels))\n",
    "labels = np.hstack((labels, a_labels))\n",
    "\n",
    "assert labels.shape[0] == n\n",
    "data[0:10], labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehDSGzojw_p7",
    "outputId": "0e33c46a-e21b-47dd-8b2e-aa7a6204cf73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14068, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "assert labels.shape[0] == n\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xaKekjHplYgw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['close to you murmured mitya kissing her dress her bosom her hands and suddenly he had a strange fancy it seemed to him that she was looking straight before her not at him not into his face but over his head with an intent almost uncanny fixity an expression of wonder almost of alarm came suddenly into her face',\n",
       "       'an hours complete leisure for such reflections as these on a dark november day a small thick rain almost blotting out the very few objects ever to be discerned from the windows was enough to make the sound of lady russells carriage exceedingly welcome and yet though desirous to be gone she could not quit the mansion house or look an adieu to the cottage with its black dripping and comfortless veranda or even notice through the misty glasses the last humble tenements of the village without a saddened heart scenes had passed in uppercross which made it precious it stood the record of many sensations of pain once severe but now softened and of some instances of relenting feeling some breathings of friendship and reconciliation which could never be looked for again and which could never cease to be dear she left it all behind her all but the recollection that such things had been',\n",
       "       'all the agreeable of her speculation was over for that hour it was time to have done with cards if sermons prevailed and she was glad to find it necessary to come to a conclusion and be able to refresh her spirits by a change of place and neighbour',\n",
       "       'here a d tremendous rap interrupted my father in his speech and somewhat alarmed my mother and me',\n",
       "       'no i dare say nor if he were to be gone a twelvemonth would you ever write to him nor he to you if it could be helped the occasion would never be foreseen what strange creatures brothers are you would not write to each other but upon the most urgent necessity in the world and when obliged to take up the pen to say that such a horse is ill or such a relation dead it is done in the fewest possible words you have but one style among you i know it perfectly henry who is in every other respect exactly what a brother should be who loves me consults me confides in me and will talk to me by the hour together has never yet turned the page in a letter and very often it is nothing more than dear mary i am just arrived bath seems full and everything as usual yours sincerely that is the true manly style that is a complete brothers letter'],\n",
       "      dtype='<U14176')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(data.shape[0] * .8)\n",
    "n_val = int(data.shape[0] * .2)\n",
    "\n",
    "indices = np.random.permutation(range(data.shape[0]))\n",
    "train_indices = indices[:n_train-n_val]\n",
    "val_indices = indices[n_train-n_val:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "X_train, y_train = data[train_indices], labels[train_indices]\n",
    "X_val, y_val = data[val_indices], labels[val_indices]\n",
    "X_test, y_test = data[test_indices], labels[test_indices]\n",
    "\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == n\n",
    "assert y_train.shape[0] + y_val.shape[0] + y_test.shape[0] == n\n",
    "\n",
    "X_train[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZls58QkMHq9",
    "outputId": "5fcab5d0-27a2-4d27-fe01-60f22a81ee71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max(len(line.split()) for line in data)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DPL-yoZnMkr2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "seqs = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEaX3lqdb0NT",
    "outputId": "0a0de639-907f-433c-c16a-79f83ed3fcf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8441, 2681),\n",
       " array([[  43,    8,    5, ...,    0,    0,    0],\n",
       "        [ 214,   65,   78, ...,    0,    0,    0],\n",
       "        [ 291, 1693,  926, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  43,   24,    1, ...,    0,    0,    0],\n",
       "        [   7,    6,  219, ...,    0,    0,    0],\n",
       "        [   7, 6960,   21, ...,    0,    0,    0]], dtype=int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape, X_train_pad[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J980IHl_aosP",
    "outputId": "404e0f34-938c-4fb5-c0ae-c18c83d86fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2813, 2681), (2813, 2681))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "X_val_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FmLT2TKcBYU",
    "outputId": "2b160fb9-fa54-4e9f-9040-67dbe71c1311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-23 22:18:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-04-23 22:18:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-04-23 22:18:53--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘/tmp/glove.6B.zip’\n",
      "\n",
      "/tmp/glove.6B.zip   100%[===================>] 822.24M  5.20MB/s    in 2m 40s  \n",
      "\n",
      "2021-04-23 22:21:33 (5.13 MB/s) - ‘/tmp/glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -O /tmp/glove.6B.zip\n",
    "!unzip -q /tmp/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fn2tW6vdVDr",
    "outputId": "f7f168d6-1d50-4216-8fec-4c029ad7302c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"/tmp/glove.6B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yf4-6G4fgBjq",
    "outputId": "296e71db-9c07-4ee5-c062-9b4d25a9f774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 16805 words (1162 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BN93KSCshenS",
    "outputId": "7aed73ae-f9e8-46b3-979a-48acc55b8346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2681, 100)         1796900   \n",
      "_________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1 (None, 2687, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 2684, 128)         51328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,856,679\n",
      "Trainable params: 59,779\n",
      "Non-trainable params: 1,796,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Conv1D, ZeroPadding1D, Dense, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(\n",
    "    Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "    )\n",
    ")\n",
    "cnn_model.add(ZeroPadding1D(3))\n",
    "cnn_model.add(Conv1D(128, 4, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(3, activation='softmax'))\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T44lWP4jwlTb",
    "outputId": "650702af-fd70-47f6-c89b-5fca8b205534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 7s 15ms/step - loss: 0.5854 - acc: 0.7635 - precision: 0.7924 - recall: 0.7266 - val_loss: 0.2373 - val_acc: 0.9097 - val_precision: 0.9361 - val_recall: 0.8905\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.1583 - acc: 0.9436 - precision: 0.9554 - recall: 0.9324 - val_loss: 0.1948 - val_acc: 0.9271 - val_precision: 0.9402 - val_recall: 0.9172\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0983 - acc: 0.9687 - precision: 0.9729 - recall: 0.9616 - val_loss: 0.3087 - val_acc: 0.9019 - val_precision: 0.9074 - val_recall: 0.8987\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0546 - acc: 0.9835 - precision: 0.9849 - recall: 0.9817 - val_loss: 0.2384 - val_acc: 0.9239 - val_precision: 0.9278 - val_recall: 0.9186\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0240 - acc: 0.9957 - precision: 0.9965 - recall: 0.9947 - val_loss: 0.2197 - val_acc: 0.9293 - val_precision: 0.9343 - val_recall: 0.9253\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0183 - acc: 0.9968 - precision: 0.9968 - recall: 0.9967 - val_loss: 0.3102 - val_acc: 0.9207 - val_precision: 0.9240 - val_recall: 0.9200\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0121 - acc: 0.9979 - precision: 0.9983 - recall: 0.9979 - val_loss: 0.2864 - val_acc: 0.9278 - val_precision: 0.9299 - val_recall: 0.9250\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0155 - acc: 0.9974 - precision: 0.9976 - recall: 0.9974 - val_loss: 0.2517 - val_acc: 0.9328 - val_precision: 0.9336 - val_recall: 0.9303\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0105 - acc: 0.9978 - precision: 0.9978 - recall: 0.9977 - val_loss: 0.3470 - val_acc: 0.9093 - val_precision: 0.9107 - val_recall: 0.9065\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 3s 13ms/step - loss: 0.0423 - acc: 0.9836 - precision: 0.9839 - recall: 0.9827 - val_loss: 0.2746 - val_acc: 0.9310 - val_precision: 0.9326 - val_recall: 0.9300\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import Precision, Recall\n",
    "from keras.optimizers import Adam\n",
    "cnn_model.compile(optimizer=Adam(lr=1E-3), loss='categorical_crossentropy', \n",
    "                  metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "history = cnn_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                        batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_H3mKQDktQG",
    "outputId": "7070d62f-6099-43db-d590-22c1248f925e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2681, 100)         1796900   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 2681, 256)         365568    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,195,751\n",
      "Trainable params: 398,851\n",
      "Non-trainable params: 1,796,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "))\n",
    "lstm_model.add(LSTM(256, return_sequences=True))\n",
    "lstm_model.add(GlobalMaxPooling1D())\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "lstm_model.add(Dense(3, activation='softmax'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD0ruqcM1nh3",
    "outputId": "1bb25fa2-5ec6-448c-bf27-e65d3465189c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 27s 99ms/step - loss: 0.6474 - acc: 0.7291 - precision_1: 0.7707 - recall_1: 0.6894 - val_loss: 0.2344 - val_acc: 0.9086 - val_precision_1: 0.9315 - val_recall_1: 0.8891\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.2054 - acc: 0.9229 - precision_1: 0.9393 - recall_1: 0.9096 - val_loss: 0.1851 - val_acc: 0.9264 - val_precision_1: 0.9397 - val_recall_1: 0.9140\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 26s 98ms/step - loss: 0.1434 - acc: 0.9466 - precision_1: 0.9559 - recall_1: 0.9375 - val_loss: 0.1682 - val_acc: 0.9353 - val_precision_1: 0.9442 - val_recall_1: 0.9257\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.0904 - acc: 0.9676 - precision_1: 0.9744 - recall_1: 0.9626 - val_loss: 0.1695 - val_acc: 0.9332 - val_precision_1: 0.9382 - val_recall_1: 0.9289\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 26s 98ms/step - loss: 0.0494 - acc: 0.9863 - precision_1: 0.9886 - recall_1: 0.9847 - val_loss: 0.2057 - val_acc: 0.9278 - val_precision_1: 0.9336 - val_recall_1: 0.9253\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 26s 98ms/step - loss: 0.0324 - acc: 0.9910 - precision_1: 0.9913 - recall_1: 0.9898 - val_loss: 0.1774 - val_acc: 0.9389 - val_precision_1: 0.9424 - val_recall_1: 0.9357\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.0203 - acc: 0.9946 - precision_1: 0.9958 - recall_1: 0.9944 - val_loss: 0.2017 - val_acc: 0.9349 - val_precision_1: 0.9395 - val_recall_1: 0.9328\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 26s 98ms/step - loss: 0.0104 - acc: 0.9982 - precision_1: 0.9982 - recall_1: 0.9979 - val_loss: 0.2566 - val_acc: 0.9314 - val_precision_1: 0.9323 - val_recall_1: 0.9300\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.0118 - acc: 0.9974 - precision_1: 0.9974 - recall_1: 0.9973 - val_loss: 0.2790 - val_acc: 0.9303 - val_precision_1: 0.9316 - val_recall_1: 0.9300\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 26s 99ms/step - loss: 0.0054 - acc: 0.9989 - precision_1: 0.9989 - recall_1: 0.9989 - val_loss: 0.2275 - val_acc: 0.9374 - val_precision_1: 0.9387 - val_recall_1: 0.9360\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=Adam(lr=1E-3), loss='categorical_crossentropy', \n",
    "                  metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "history = lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                         batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mbk74vnVldsV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 2681, 100)         1796900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,195,751\n",
      "Trainable params: 398,851\n",
      "Non-trainable params: 1,796,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "))\n",
    "lstm_model.add(LSTM(256, return_sequences=False))\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "lstm_model.add(Dense(3, activation='softmax'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 27s 99ms/step - loss: 1.0677 - acc: 0.6082 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.9946 - val_acc: 0.6100 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 25s 96ms/step - loss: 0.9789 - acc: 0.6096 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00 - val_loss: 0.9413 - val_acc: 0.6100 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.9337 - acc: 0.6147 - precision_2: 0.5976 - recall_2: 0.5527 - val_loss: 0.9155 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 25s 96ms/step - loss: 0.9144 - acc: 0.6119 - precision_2: 0.6119 - recall_2: 0.6119 - val_loss: 0.9033 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 25s 96ms/step - loss: 0.9028 - acc: 0.6134 - precision_2: 0.6134 - recall_2: 0.6134 - val_loss: 0.8974 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 6/10\n",
      "264/264 [==============================] - 25s 96ms/step - loss: 0.9007 - acc: 0.6089 - precision_2: 0.6089 - recall_2: 0.6089 - val_loss: 0.8946 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 7/10\n",
      "264/264 [==============================] - 25s 96ms/step - loss: 0.9002 - acc: 0.6118 - precision_2: 0.6118 - recall_2: 0.6118 - val_loss: 0.8932 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 8/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.9026 - acc: 0.6085 - precision_2: 0.6085 - recall_2: 0.6085 - val_loss: 0.8924 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 9/10\n",
      "264/264 [==============================] - 26s 97ms/step - loss: 0.9018 - acc: 0.6068 - precision_2: 0.6068 - recall_2: 0.6068 - val_loss: 0.8921 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n",
      "Epoch 10/10\n",
      "264/264 [==============================] - 26s 98ms/step - loss: 0.8958 - acc: 0.6158 - precision_2: 0.6158 - recall_2: 0.6158 - val_loss: 0.8919 - val_acc: 0.6100 - val_precision_2: 0.6100 - val_recall_2: 0.6100\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=Adam(lr=1E-3), loss='categorical_crossentropy', \n",
    "                  metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "history = lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                         batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8441, 17967), (2813, 17967), (2814, 17967))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def _insert_bias(X):\n",
    "    \"\"\" Inserts the bias \"\"\"\n",
    "    bias = np.ones((X.shape[0], 1))\n",
    "    if isinstance(X, scipy.sparse.csr.csr_matrix):\n",
    "        X = np.concatenate((X.todense(), bias), axis=1)  # Insert bias into the features\n",
    "        X = csr_matrix(X)\n",
    "    else:\n",
    "        X = np.concatenate((X, bias), axis=1)\n",
    "    return X\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=tokenizer.word_index.keys())\n",
    "tf_train = vectorizer.fit_transform(X_train)\n",
    "tf_val = vectorizer.transform(X_val)\n",
    "tf_test = vectorizer.transform(X_test)\n",
    "\n",
    "assert tf_train.shape[0] + tf_val.shape[0] + tf_test.shape[0] == n\n",
    "\n",
    "tf_train.shape, tf_val.shape, tf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8441, 17968), (2813, 17968), (2814, 17968))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train = _insert_bias(tf_train)\n",
    "tf_val = _insert_bias(tf_val)\n",
    "tf_test = _insert_bias(tf_test)\n",
    "\n",
    "tf_train.shape, tf_val.shape, tf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate: float, max_epochs: int = 1000, precision: float = 1E-6, \n",
    "                 lam: float = 1E-4, optimizer: str = 'sgd', batch_size: int = None):\n",
    "        \"\"\" Initializes the class \"\"\"\n",
    "        self.rate = learning_rate\n",
    "        self.epochs = max_epochs\n",
    "        self.precision = precision\n",
    "        self.lambda_ = lam\n",
    "        self.b_ = batch_size\n",
    "        self.alg_ = self._set_optimizer(optimizer)\n",
    "        self.loss_ = None\n",
    "        self.val_loss_ = None\n",
    "        self.w_ = None\n",
    "        \n",
    "    def _set_optimizer(self, optimizer: str):\n",
    "        \"\"\" Sets the optimizer to either stochastic or mini-batch stochastic gradient descent \"\"\"\n",
    "        optimizer = optimizer.lower().strip()\n",
    "        if optimizer == 'sgd':\n",
    "            return self._stochastic_descent\n",
    "        elif optimizer == 'mbsgd':\n",
    "            if not self.b_:\n",
    "                raise ValueError(\"You must declare a batch size with 'batch_size' in order to use \" \\\n",
    "                                 \"mini-batch stochastic gradient descent\")\n",
    "            return self._mb_stochastic_descent\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer must be of value 'sgd' or 'mbsgd'. The value {optimizer} is not valid\")\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predicts the class of the input array \"\"\"\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1, X.shape[0]))\n",
    "        phi = X.dot(self.w_)\n",
    "        return np.argmax(self._softmax_batch(phi), axis=1)\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\" Scores how well the Logistic model predicts \"\"\"\n",
    "        labels = self.predict(X)\n",
    "        onehot = np.argmax(y, axis=1)\n",
    "        return sum(labels == onehot) / len(y)\n",
    "    \n",
    "    def plot(self) -> None:\n",
    "        \"\"\" Plots the loss \"\"\"\n",
    "        epochs = len(self.loss_)\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epochs), self.loss_)\n",
    "        if self.val_loss_:\n",
    "            plt.plot(range(epochs), self.val_loss_)\n",
    "        plt.title('Loss per Epoch')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, X_val: np.ndarray = None, y_val: np.ndarray = None,\n",
    "            verbose: bool = True) -> None:\n",
    "        \"\"\" Fits the model to the training data \"\"\"\n",
    "        self.loss_ = list()\n",
    "        self.val_loss_ = list()\n",
    "        for i in range(self.epochs):\n",
    "            n, d = X.shape\n",
    "            rand_indices = np.random.permutation(n)\n",
    "            xi = X[rand_indices, :]\n",
    "            yi = y[rand_indices, :]\n",
    "            \n",
    "            if self.w_ is None:\n",
    "                self.w_ = np.random.normal(scale = 0.1, size = (d, yi.shape[1])) # d x k matrix\n",
    "            \n",
    "            loss = self.alg_(xi, yi)\n",
    "            self.loss_.append(loss)\n",
    "            \n",
    "            val_loss = None\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss = self.loss(X_val, y_val)\n",
    "                self.val_loss_.append(val_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Loss at epoch {i} is {loss}\")\n",
    "                if val_loss:\n",
    "                    print(f\"Val Loss at epoch {i} is {val_loss}\")\n",
    "            \n",
    "            if i > 0 and abs(self.loss_[i-1] - loss) <= self.precision:\n",
    "                print(f\"Precision reached at epoch {i}\")\n",
    "                break\n",
    "                \n",
    "            self.rate *= 0.9  # Decay the learning rate for stability\n",
    "            \n",
    "    def loss(self, X, y) -> float:\n",
    "        \"\"\" Defines the loss for the model \"\"\"\n",
    "        phi = X.dot(self.w_)  # 1 x k vector\n",
    "        softmax = self._softmax_batch(phi)  # 1 x k vector\n",
    "        loss = np.sum(np.multiply(y, np.log(softmax))) \n",
    "        loss += self.lambda_ * np.sum(np.square(self.w_))  # scalar\n",
    "        return -loss / X.shape[0]\n",
    "        \n",
    "    def _stochastic_descent(self, xi: np.ndarray, yi: np.ndarray) -> float:\n",
    "        \"\"\" Implementation of stochastic gradient descent \"\"\"\n",
    "        n, d = xi.shape\n",
    "        epoch_loss = 0.\n",
    "        for k in range(n):\n",
    "            x = xi[k, :]\n",
    "            y = yi[k, :]\n",
    "            loss, g = self._stochastic_gradient(x, y)\n",
    "            epoch_loss += loss\n",
    "            self.w_ -= self.rate * g\n",
    "        return -epoch_loss / n\n",
    "        \n",
    "    def _stochastic_gradient(self, x: np.ndarray, y: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\" Computes the stochastic gradient and its loss \"\"\"\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape((1, x.shape[0]))  # 1 x d vector\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape((1, y.shape[0]))  # 1 x k vector\n",
    "        \n",
    "        phi = x.dot(self.w_)  # 1 x k vector\n",
    "        softmax = self._softmax(phi)  # 1 x k vector\n",
    "        loss = np.sum(np.multiply(y, np.log(softmax))) + (self.lambda_ * np.sum(np.square(self.w_)))  # scalar     \n",
    "        g = (x.T.dot(softmax - y)) + (2 * self.lambda_ * self.w_) # d x k matrix\n",
    "        return loss, g\n",
    "    \n",
    "    @staticmethod\n",
    "    def _softmax(phi: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Solves the softmax function \"\"\"\n",
    "        exps = np.exp(phi)\n",
    "        return exps / np.sum(exps)  # k x 1 matrix\n",
    "\n",
    "    def _mb_stochastic_descent(self, xi: np.ndarray, yi: np.ndarray) -> float:\n",
    "        \"\"\" Implementation of stochastic gradient descent \"\"\"\n",
    "        n, d = xi.shape\n",
    "        iters = int(n / self.b_)\n",
    "        epoch_loss = 0.\n",
    "        start = 0\n",
    "        for k in range(iters):\n",
    "            end = start + self.b_\n",
    "            x = xi[start:end, :]\n",
    "            y = yi[start:end, :]\n",
    "            loss, g = self._mb_stochastic_gradient(x, y)\n",
    "            epoch_loss += loss\n",
    "            self.w_ -= self.rate * g\n",
    "            start = end\n",
    "        return -epoch_loss / iters\n",
    "        \n",
    "    def _mb_stochastic_gradient(self, x: np.ndarray, y: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\" Computes the mini-batch stochastic gradient and its loss \"\"\"\n",
    "        phi = x.dot(self.w_)  # b x k matrix\n",
    "        softmax = self._softmax_batch(phi)  # b x k matrix\n",
    "        \n",
    "        loss = np.sum(np.multiply(y, np.log(softmax))) \n",
    "        loss += self.lambda_ * np.sum(np.square(self.w_))  # scalar\n",
    "        loss /= self.b_\n",
    "        \n",
    "        g = x.T.dot(softmax - y) + (2 * self.lambda_ * self.w_) # d x k matrix\n",
    "        g /= self.b_\n",
    "        return loss, g\n",
    "\n",
    "    @staticmethod\n",
    "    def _softmax_batch(phi: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Defines row-wise softmax \"\"\"\n",
    "        exps = np.exp(phi)\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 is 0.536602936237428\n",
      "Val Loss at epoch 0 is 0.450591824035884\n",
      "Loss at epoch 1 is 0.3218058211278821\n",
      "Val Loss at epoch 1 is 0.3790897945018713\n",
      "Loss at epoch 2 is 0.2503220439486167\n",
      "Val Loss at epoch 2 is 0.3416193783457667\n",
      "Loss at epoch 3 is 0.21224904751487908\n",
      "Val Loss at epoch 3 is 0.32243412768507007\n",
      "Loss at epoch 4 is 0.18932884789004903\n",
      "Val Loss at epoch 4 is 0.31236847475850493\n",
      "Loss at epoch 5 is 0.17456071878521004\n",
      "Val Loss at epoch 5 is 0.30564413360740683\n",
      "Loss at epoch 6 is 0.1636535583013369\n",
      "Val Loss at epoch 6 is 0.3030836601393823\n",
      "Loss at epoch 7 is 0.156195549059487\n",
      "Val Loss at epoch 7 is 0.29854410136265613\n",
      "Loss at epoch 8 is 0.15032818516529883\n",
      "Val Loss at epoch 8 is 0.29646735814822056\n",
      "Loss at epoch 9 is 0.14550591119685274\n",
      "Val Loss at epoch 9 is 0.29655564806016316\n"
     ]
    }
   ],
   "source": [
    "sgdlog = LogisticRegression(0.1, max_epochs=10)\n",
    "sgdlog.fit(tf_train, y_train, tf_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgdlog.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the Neuron \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, xi):\n",
    "        \"\"\" Defines the foward pass of the neuron \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, xi, gradient, rate):\n",
    "        \"\"\" Defines the backward pass for the neuron \"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "class ReLU(Neuron):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the Neuron \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, xi):\n",
    "        \"\"\" Defines the ReLU function \"\"\"\n",
    "        return np.maximum(0, xi)\n",
    "    \n",
    "    def backward(self, xi, gradient, rate):\n",
    "        \"\"\" Defines the gradient for the ReLU function \"\"\"\n",
    "        relu_g = xi > 0\n",
    "        backpass = gradient * relu_g\n",
    "        return backpass\n",
    "        \n",
    "\n",
    "class Tanh(Neuron):\n",
    "    \n",
    "    \"\"\" This was a thought, but it was not actually used \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the Neuron \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, xi):\n",
    "        \"\"\" Defines the Tanh function \"\"\"\n",
    "        return np.tanh(xi)\n",
    "    \n",
    "    def backward(self, xi, gradient, rate):\n",
    "        \"\"\" Defines the gradient for the Tanh function \"\"\"\n",
    "        backpass = (1 - np.square(np.tanh(xi))) * gradient\n",
    "        return backpass\n",
    "    \n",
    "\n",
    "class Softmax(Neuron):\n",
    "     \n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the Neuron \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, xi):\n",
    "        \"\"\" Defines the Softmax function \"\"\"\n",
    "        exps = np.exp(xi - np.max(xi))  # Adding np.max(xi) for numeric stability\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, xi, gradient, rate):\n",
    "        \"\"\" Defines the gradient for the Softmax function \"\"\"\n",
    "        sm = self.forward(xi) \n",
    "        backpass = (sm - gradient) / xi.shape[0]  # Assume the gradient is one-hot encoded y\n",
    "        return backpass\n",
    "    \n",
    "\n",
    "class Layer(Neuron):\n",
    "    \n",
    "    def __init__(self, xdim: int, ydim: int):\n",
    "        \"\"\" Initializes the class \"\"\"\n",
    "        # Bias is assumed to be a feature for simplification purposes\n",
    "        self._he_init = np.sqrt((2 / (xdim + ydim)))  # Initialize the weights with He initialization scheme\n",
    "        self.w_ = np.random.normal(scale = self._he_init, size = (xdim, ydim))\n",
    "        \n",
    "    def forward(self, xi) -> np.ndarray:\n",
    "        \"\"\" Performs the linear matrix operation for the layer \"\"\"\n",
    "        return xi.dot(self.w_)\n",
    "    \n",
    "    def backward(self, xi, gradient, rate):\n",
    "        \"\"\" Performs the backward pass for the layer \"\"\"\n",
    "        # Mini-batch stochastic descent\n",
    "        cost = xi.T.dot(gradient) / xi.shape[0]  # Assumes mini-batch stochastic gradient-descent \n",
    "        self.w_ -= rate * cost\n",
    "        \n",
    "        # Gradient for Input\n",
    "        xi_gradient = gradient.dot(self.w_.T)\n",
    "        return xi_gradient\n",
    "    \n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.1, max_epochs: int = 1000, precision: float = 1E-6, \n",
    "                 batch_size: int = None):\n",
    "        \"\"\" Initializes the Network \"\"\"\n",
    "        self.network = []\n",
    "        self.rate = learning_rate\n",
    "        self.epochs = max_epochs\n",
    "        self.precision = precision\n",
    "        self.b_ = batch_size\n",
    "        self.loss_ = None\n",
    "        self.val_loss_ = None\n",
    "        self.get_loss = None\n",
    "        self.get_gradient = None\n",
    "    \n",
    "    def add_layer(self, layer) -> None:\n",
    "        \"\"\" Adds a layer to the network \"\"\"\n",
    "        self.network.append(layer)\n",
    "    \n",
    "    def loss(self, func, func_prime):\n",
    "        \"\"\" Sets the loss to use for the network \"\"\"\n",
    "        self.get_loss = func\n",
    "        self.get_gradient = func_prime\n",
    "            \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        \"\"\" Defines the predict function \"\"\"\n",
    "        results = self._feed_forward(X)\n",
    "        return results[-1]\n",
    "    \n",
    "    def score(self, X, y) -> float:\n",
    "        \"\"\" Scores the predictive ability of the Network \"\"\"\n",
    "        results = self.predict(X)\n",
    "        labels = np.argmax(results, axis=1)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        return sum(labels == y) / len(labels)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, X_val: np.ndarray = None, y_val: np.ndarray = None,\n",
    "            verbose: bool = True):\n",
    "        \"\"\" Fits the network to the training vectors \"\"\"\n",
    "        self.loss_ = list()\n",
    "        self.val_loss_ = list()\n",
    "        for i in range(self.epochs):\n",
    "            n, d = X.shape\n",
    "            rand_indices = np.random.permutation(n)\n",
    "            xi = X[rand_indices, :]\n",
    "            yi = y[rand_indices, :]\n",
    "            \n",
    "            loss = self._train(xi, yi)\n",
    "            self.loss_.append(loss)\n",
    "            \n",
    "            val_loss = None\n",
    "            if X_val is not None and y_val is not None:\n",
    "                preds = self.predict(X_val)\n",
    "                val_loss = -self.get_loss(preds, y_val)\n",
    "                self.val_loss_.append(val_loss)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Loss at epoch {i} is {loss}\")\n",
    "                if val_loss:\n",
    "                    print(f\"Val Loss at epoch {i} is {val_loss}\")\n",
    "            \n",
    "            if i > 0 and abs(self.loss_[i-1] - loss) <= self.precision:\n",
    "                print(f\"Precision reached at epoch {i}\")\n",
    "                break\n",
    "    \n",
    "    def plot(self) -> None:\n",
    "        \"\"\" Plots the loss \"\"\"\n",
    "        epochs = len(self.loss_)\n",
    "        fig = plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epochs), self.loss_)\n",
    "        if self.val_loss_:\n",
    "            plt.plot(range(epochs), self.val_loss_)\n",
    "        plt.title('Loss per Epoch')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "    \n",
    "    def _train(self, xi: np.ndarray, yi: np.ndarray) -> float:\n",
    "        \"\"\" Implements mini-batch stochastic gradient descent \"\"\"\n",
    "        n, _ = xi.shape\n",
    "        iters = int(n / self.b_)\n",
    "        epoch_loss = 0.\n",
    "        start = 0\n",
    "        for k in range(iters):\n",
    "            end = start + self.b_\n",
    "            x = xi[start:end, :]\n",
    "            y = yi[start:end, :]\n",
    "            outputs = self._feed_forward(x)\n",
    "            result = outputs[-1]\n",
    "            \n",
    "            loss = self.get_loss(result, y)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            g = self.get_gradient(result, y)\n",
    "            self._backward_pass(outputs, g)\n",
    "            start = end\n",
    "        return -epoch_loss / iters\n",
    "        \n",
    "    def _feed_forward(self, xi) -> np.ndarray:\n",
    "        \"\"\" Feeds the sample forward through the network \"\"\"\n",
    "        outputs = [None] * (len(self.network) + 1)\n",
    "        outputs[0] = xi\n",
    "        for i, layer in enumerate(self.network):\n",
    "            xi = layer.forward(xi)\n",
    "            outputs[i + 1] = xi\n",
    "        return outputs\n",
    "    \n",
    "    def _backward_pass(self, outputs, gradient) -> None:\n",
    "        \"\"\" Implements a backward pass through the network \"\"\"\n",
    "        for i in range(len(outputs) - 1)[::-1]:\n",
    "            xi = outputs[i]\n",
    "            layer = self.network[i]\n",
    "            gradient = layer.backward(xi, gradient, self.rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x, y) -> float:\n",
    "    \"\"\" Returns the loss of cross entropy logistic regression \"\"\"\n",
    "    loss = np.sum(np.multiply(y, np.log(x))) / x.shape[0]\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_loss_g(x, y) -> np.ndarray:\n",
    "    \"\"\" Returns gradient of loss for network \"\"\"\n",
    "    return y  # Only return y because the output layer has the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(xdim: int, ydim: int, epochs: int = 30, batch_size = 12, neurons: int = 50) -> tuple:\n",
    "    \"\"\" Gets everything ready to start \"\"\"\n",
    "    net = Network(0.1, max_epochs = epochs, batch_size = batch_size)\n",
    "    net.add_layer(Layer(xdim, neurons))\n",
    "    net.add_layer(ReLU())\n",
    "    net.add_layer(Layer(neurons, ydim))\n",
    "    net.add_layer(Softmax())\n",
    "    net.loss(cross_entropy_loss, cross_entropy_loss_g)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 is 0.9334193230465163\n",
      "Val Loss at epoch 0 is 0.8771864163578239\n",
      "Loss at epoch 1 is 0.8683628656825066\n",
      "Val Loss at epoch 1 is 0.8565615129964343\n",
      "Loss at epoch 2 is 0.847777392702398\n",
      "Val Loss at epoch 2 is 0.834220833062421\n",
      "Loss at epoch 3 is 0.8220085634377051\n",
      "Val Loss at epoch 3 is 0.8059114957205875\n",
      "Loss at epoch 4 is 0.7901032447112497\n",
      "Val Loss at epoch 4 is 0.7726117623764177\n",
      "Loss at epoch 5 is 0.7515885498547554\n",
      "Val Loss at epoch 5 is 0.7300621954387776\n",
      "Loss at epoch 6 is 0.7080881752431525\n",
      "Val Loss at epoch 6 is 0.6846287599651413\n",
      "Loss at epoch 7 is 0.662309170479113\n",
      "Val Loss at epoch 7 is 0.6405366960295249\n",
      "Loss at epoch 8 is 0.6177167652624634\n",
      "Val Loss at epoch 8 is 0.5983260169287447\n",
      "Loss at epoch 9 is 0.5769404054540711\n",
      "Val Loss at epoch 9 is 0.5593158403350472\n"
     ]
    }
   ],
   "source": [
    "net = build_network(tf_train.shape[1], y_train.shape[1], epochs = 10, neurons = 50)\n",
    "net.fit(tf_train, y_train, tf_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(path: str):\n",
    "    \"\"\" Loads the file into memory \"\"\"\n",
    "    with open(path, 'r', encoding='ISO-8859-1') as fo:\n",
    "        content = fo.read()\n",
    "    return content\n",
    "\n",
    "def clean_data(lines: list):\n",
    "    \"\"\" Cleans the data for tokenization \"\"\"\n",
    "    cleaned = list()\n",
    "    for line in lines:\n",
    "        # Lowercase text and remove leading and ending newlines\n",
    "        line = line.lower().strip('\\n')\n",
    "        # Fix any encoding issues\n",
    "        line = normalize('NFD', line).encode('utf8')\n",
    "        line = line.decode('utf8')\n",
    "        # Remove new line symbols\n",
    "        line = line.replace('\\n', ' ')\n",
    "        # Remove special characters and numbers\n",
    "        line = re.sub(\"[^a-z\\s\\']+\", \" \", line).replace(\"'\", \"\")\n",
    "        # Remove whitespace\n",
    "        line = ' '.join(line.split())\n",
    "        cleaned.append(line)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_match = re.compile(r\"((?:\\<review_text\\>)([\\s\\w\\S]+?)(?:\\<\\/review_text\\>))\")\n",
    "\n",
    "pos = load_file('q2/positive.review')\n",
    "neg = load_file('q2/negative.review')\n",
    "pos_reviews = [r.group(2) for r in review_match.finditer(pos)]\n",
    "neg_reviews = [r.group(2) for r in review_match.finditer(neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " 1000,\n",
       " [\"\\nBridget Jones, modern day woman, brillant and doesn't know it, prone to accidents and mess ups but manages to come out of them.  \\n\\nThis is the book that started it all with the chick lit fever.  Bridget Jones is my hero! \\n\",\n",
       "  \"\\nI am ordering copies for all 23 middle school principals and the two assistant principals leading two middle school programs in the Milwaukee Public Schools system. We will use Wheatley's book as the primary resource  for our professional growth at our MPS Middle School Principals  Collaborative institute August 9-11, 1999. We are not just concerned with  reform; we seek renewal as well. Wheatley provides the basis. She notes  that Einstein said that  a problem cannot be solved from the same  consciousness that created it. The entire book is a marvelous exploration  of this philosophy\\n\",\n",
       "  '\\nAs a casual piano player and a Broadway fanatic, I was so jazzed to play some of the songs from Avenue Q. The book contains everything you find on the CD and includes a few production photos.\\nOf course the little details are fun, using terms like \"manilowesque\" \"Huey-Lewis shuffle\" \"prissy sonata\" and \"funky a** groove\" to describe some of the songs. Fun fun fu\\n',\n",
       "  '\\nThis is one of the best biographies I have ever read. You can tell the authors put a lot of time and effort into this work - it\\'s a true labor of love. Filled with beautiful photos and extensive bibliographical notes, this one is a keeper. Who knew Miss Francis was such a \"wild child\"? Whether you\\'re a film scholar or a movie buff, Lynn Kear\\'s book deserves a special spot in your bookcase.\\n',\n",
       "  \"\\nI read this book many, many years ago on a very long flight.  I couldn't put it down.  The philosophy is simple; live frugally, save/invest money every month, and you too can be a millionaire.  It's great advice, especially in this day and age when the younger generation wants the big house, expensive car, and everything else right NOW, with no regard to the future.  Although some of the info is dated (Enron being named as a great company to invest), and some other advice (putting new soles on old shoes) is a bit much, the general message makes plenty of sense.\\n\"])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_reviews), len(neg_reviews), pos_reviews[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (1000,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cleaned = clean_data(pos_reviews)\n",
    "neg_cleaned = clean_data(neg_reviews)\n",
    "\n",
    "pos_labels = np.ones(len(pos_cleaned))\n",
    "neg_labels = np.zeros(len(neg_cleaned))\n",
    "\n",
    "pos_cleaned.shape, neg_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['close to you murmured mitya kissing her dress her bosom her hands and suddenly he had a strange fancy it seemed to him that she was looking straight before her not at him not into his face but over his head with an intent almost uncanny fixity an expression of wonder almost of alarm came suddenly into her face',\n",
       "       'an hours complete leisure for such reflections as these on a dark november day a small thick rain almost blotting out the very few objects ever to be discerned from the windows was enough to make the sound of lady russells carriage exceedingly welcome and yet though desirous to be gone she could not quit the mansion house or look an adieu to the cottage with its black dripping and comfortless veranda or even notice through the misty glasses the last humble tenements of the village without a saddened heart scenes had passed in uppercross which made it precious it stood the record of many sensations of pain once severe but now softened and of some instances of relenting feeling some breathings of friendship and reconciliation which could never be looked for again and which could never cease to be dear she left it all behind her all but the recollection that such things had been',\n",
       "       'all the agreeable of her speculation was over for that hour it was time to have done with cards if sermons prevailed and she was glad to find it necessary to come to a conclusion and be able to refresh her spirits by a change of place and neighbour',\n",
       "       'here a d tremendous rap interrupted my father in his speech and somewhat alarmed my mother and me',\n",
       "       'no i dare say nor if he were to be gone a twelvemonth would you ever write to him nor he to you if it could be helped the occasion would never be foreseen what strange creatures brothers are you would not write to each other but upon the most urgent necessity in the world and when obliged to take up the pen to say that such a horse is ill or such a relation dead it is done in the fewest possible words you have but one style among you i know it perfectly henry who is in every other respect exactly what a brother should be who loves me consults me confides in me and will talk to me by the hour together has never yet turned the page in a letter and very often it is nothing more than dear mary i am just arrived bath seems full and everything as usual yours sincerely that is the true manly style that is a complete brothers letter'],\n",
       "      dtype='<U14176')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(pos_cleaned.shape[0] * .8)\n",
    "n_val = int(pos_cleaned.shape[0] * .2)\n",
    "\n",
    "indices = np.random.permutation(range(pos_cleaned.shape[0]))\n",
    "train_indices = indices[:n_train-n_val]\n",
    "val_indices = indices[n_train-n_val:n_train]\n",
    "test_indices = indices[n_train:]\n",
    "\n",
    "assert train_indices.shape[0] + val_indices.shape[0] + test_indices.shape[0] == 1000\n",
    "\n",
    "pos_train, y_pos_train = pos_cleaned[train_indices], pos_labels[train_indices]\n",
    "neg_train, y_neg_train = neg_cleaned[train_indices], neg_labels[train_indices]\n",
    "\n",
    "pos_val, y_pos_val = pos_cleaned[val_indices], pos_labels[val_indices]\n",
    "neg_val, y_neg_val = neg_cleaned[val_indices], neg_labels[val_indices]\n",
    "\n",
    "\n",
    "pos_test, y_pos_test = pos_cleaned[test_indices], pos_labels[test_indices]\n",
    "neg_test, y_neg_test = neg_cleaned[test_indices], neg_labels[test_indices]\n",
    "\n",
    "X_train[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['what can i say more spidey than you can shake a stick at for peanuts if you ever wished you had the patience to cut out and neatly collect all the sunday paper comics in a nice neat scrap book then this is the book for you im planning on buying the lot and thoroughly enjoying my strolls down memory lane',\n",
       "        'if your looking to increase your personal level of joy and harmony in life implement the teachings of the frames in this book manny padro salt lake city uta',\n",
       "        'i have read this book daily for almost years the original book given to me in is somewhat worn and now i require a replacement it has been and will remain my first reading each day',\n",
       "        'i have been planting flowers from seed in my home for almost years now last year i had over seedlings on multi level shelves under flourescent and natural light in my dining room boy do i need a greenhouse so this book was not a lot of new information to me a lot of what is listed can be found on the back of a seed packet but its not practical to sit on the floor of my favorite garden supply store and read every seed package for suggestions before i buy it this is important because im not interested in refrigerating my seeds weeks before i plant them or doing other such preparations required for some seeds one major thing missing from this book was photography there were drawings but im sorry to say that they were mostly a way of breaking up the monotony of the text rather than a reference tool on the good side this book did an excellent job of providing the proper germination environment and each plant had a quick reference in a shaded box that listed the type of plant annual perennial etc its hardiness zone and its flowering season this allowed me to quickly eliminate the plants that were inappropriate for my zone the regular index wasnt very useful but one wonderful addition was an index of the common amp latin names i refer to this when looking at some of my books that include the full color photography i was missing almost makes up for it almost of course there is the standard hardiness zone map but they also added a map each for the probable first and last frost dates nice also included was a directory of seed companies and their addresses all in all this book does a good job of covering the seed part but a poor job of covering the bloom part',\n",
       "        'theres no much to say about this book everything is already said and it is still the best book in the field well done nancy sead malicevic m'],\n",
       "       dtype='<U30325'),\n",
       " array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = np.hstack((pos_train, neg_train)), np.hstack((y_pos_train, y_neg_train))\n",
    "X_val, y_val = np.hstack((pos_val, neg_val)), np.hstack((y_pos_val, y_neg_val))\n",
    "X_test, y_test = np.hstack((pos_test, neg_test)), np.hstack((y_pos_test, y_neg_test))\n",
    "\n",
    "n = len(pos_cleaned) + len(neg_cleaned)\n",
    "print(X_train.shape[0] + X_val.shape[0] + X_test.shape[0])\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == n\n",
    "assert y_train.shape[0] + y_val.shape[0] + y_test.shape[0] == n\n",
    "X_train[0:5], y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2681"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max(len(line.split()) for line in data)\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "seqs = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 2681), (400, 2681))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_pad = pad_sequences(seqs, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "X_val_pad.shape, X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"/tmp/glove.6B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 15460 words (1113 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 2681, 100)         1657500   \n",
      "_________________________________________________________________\n",
      "zero_padding1d_2 (ZeroPaddin (None, 2687, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2684, 128)         51328     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,717,149\n",
      "Trainable params: 59,649\n",
      "Non-trainable params: 1,657,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(\n",
    "    Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "    )\n",
    ")\n",
    "cnn_model.add(ZeroPadding1D(3))\n",
    "cnn_model.add(Conv1D(128, 4, activation='relu'))\n",
    "cnn_model.add(GlobalMaxPooling1D())\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 1s 16ms/step - loss: 0.7251 - acc: 0.4944 - val_loss: 0.6511 - val_acc: 0.6200\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.5813 - acc: 0.7484 - val_loss: 0.6081 - val_acc: 0.6875\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.4714 - acc: 0.8294 - val_loss: 0.5644 - val_acc: 0.7175\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.3471 - acc: 0.8964 - val_loss: 0.5463 - val_acc: 0.7100\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.2542 - acc: 0.9374 - val_loss: 0.5184 - val_acc: 0.7200\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.1693 - acc: 0.9783 - val_loss: 0.4949 - val_acc: 0.7725\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 0.0826 - acc: 0.9939 - val_loss: 0.5040 - val_acc: 0.7850\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.0460 - acc: 1.0000 - val_loss: 0.5124 - val_acc: 0.7875\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 1s 13ms/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.5165 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 0s 13ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.7925\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = cnn_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                        batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 2681, 100)         1657500   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 2681, 256)         365568    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,056,093\n",
      "Trainable params: 398,593\n",
      "Non-trainable params: 1,657,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "))\n",
    "lstm_model.add(LSTM(256, return_sequences=True))\n",
    "lstm_model.add(GlobalMaxPooling1D())\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 5s 104ms/step - loss: 0.6859 - acc: 0.5623 - val_loss: 0.6633 - val_acc: 0.5950\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 4s 103ms/step - loss: 0.6437 - acc: 0.6179 - val_loss: 0.5730 - val_acc: 0.7275\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.5107 - acc: 0.7684 - val_loss: 0.5028 - val_acc: 0.7775\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.4422 - acc: 0.7890 - val_loss: 0.4908 - val_acc: 0.7700\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.3053 - acc: 0.8843 - val_loss: 0.4824 - val_acc: 0.7775\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 4s 99ms/step - loss: 0.2333 - acc: 0.9214 - val_loss: 0.4381 - val_acc: 0.8150\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.1561 - acc: 0.9667 - val_loss: 0.4707 - val_acc: 0.8125\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0991 - acc: 0.9862 - val_loss: 0.5057 - val_acc: 0.7825\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0769 - acc: 0.9880 - val_loss: 0.5056 - val_acc: 0.8075\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 4s 98ms/step - loss: 0.0491 - acc: 0.9937 - val_loss: 0.5190 - val_acc: 0.8100\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                         batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 2681, 100)         1657500   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,056,093\n",
      "Trainable params: 398,593\n",
      "Non-trainable params: 1,657,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_seq_length\n",
    "))\n",
    "lstm_model.add(LSTM(256, return_sequences=False))\n",
    "lstm_model.add(Dense(128, activation='relu'))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 5s 104ms/step - loss: 0.6931 - acc: 0.5290 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6929 - acc: 0.5131 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6932 - acc: 0.5086 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6909 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 4s 96ms/step - loss: 0.6918 - acc: 0.5190 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6905 - acc: 0.5230 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6925 - acc: 0.4930 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6931 - acc: 0.5024 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6927 - acc: 0.4899 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 4s 97ms/step - loss: 0.6929 - acc: 0.4863 - val_loss: 0.6931 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "lstm_model.compile(optimizer=Adam(lr=1E-3), loss='binary_crossentropy', \n",
    "                  metrics=['acc'])\n",
    "\n",
    "history = lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val),\n",
    "                         batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
